% !TEX root = ../report.tex

\chapter{Literature Review}\label{litreview}
A literature review was undertaken on all aspects of the project including
co-operative robotics, computer vision and SLAM both in general and more
specifically using basic sensors. This was done initially using the Google
Scholar search engine to search for articles which would be of relevance to
these topics in isolation and also collectively. These articles, which mainly
consisted of a variety of journal articles and conference proceedings from
the IEEE, were then screened further based on their titles and abstracts.
Priority was given to those with direct relation to the study---for example
those using ultrasonic sensors to do SLAM or those also using ground vehicles.
Related articles were also found through citations used within these articles
in addition to the related articles feature provided by Google Scholar. These
searches resulted in full investigations of around 4 articles for each of the main
review topics. When more specific information was required e.g., for specific
algorithms related to the study, the search term was altered to refine the
relevant articles. If at any point throughout the study it was felt that more
reading was required, the selection of articles here were revisited, with
related articles used to fill in any gaps in knowledge from the original
searches.

\section{Robotics}\label{litreview/robotics}
A robot is ``a machine capable of carrying out a complex series of actions automatically''~\cite{https://en.oxforddictionaries.com/definition/robot}. Robotics is the ``branch of technology which deals with the design, construction, operation, and application''~\cite{https://www.merriam-webster.com/dictionary/robotics} of robots. Robotics combines a number of fields from mechanical, electrical and software engineering within a single system to achieve its goals. By combining the three major disciplines of artificial intelligence, operations research, and control theory, a resultant intelligent control system is created~\cite{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1103278} which can be used for a wide range of robotic applications.   

\subsection{Control}\label{litreview/robotics/control}  
As the robots in this study are homogeneous and should be able to complete tasks individually, control is limited to the control of a single robot in the complete system. Robotic mechanisms form the control system for a robot and connect the fixed parts of the robot together by joints to allow motion between fixed parts~\cite{lynch2017modern}.
Actuation of the joints, usually by motors, imparts forces on the robot which allow it to move and perform a variety of tasks~\cite{lynch2017modern}.
The movement of these ``actuators'' is influenced by a number of sensors which provide the system with information about its environment. These sensors can also influence the movement of the actuators through feedback from previous movement instructions~\cite{lynch2017modern}.    

Sensors can take many forms and provide information about internal and external environmental factors of the robot. External sensors provide information which the robot would otherwise find significantly more difficult to discover, one such example being range sensing. This can take a number of different forms including ultrasound, infra-red, Light Detection And Ranging (LIDAR) and binocular computer vision. Each of these sensors use unique methods to determine the distance from the robot to another object in its environment. Ultrasound sensors use high frequency sound waves which reflect from surfaces and return to the sensor in time t to calculate the distance to the object using the speed of sound. Infra-red sensors work in a similar fashion but with invisible light instead of sound. 

LIDAR is a more advanced use of invisible light to more accurately detect distance using more powerful and precise beams of light than in the infra-red case~\cite{LIDAR}. Binocular vision allows depth perception to take place similar to that allowed by human vision~\cite{read2005early} by perspective-based cues which can only be obtained from at least binocular vision~\cite{
pfautz2002depth}. 

Internal sensors can either be related to the actuators or fixed parts of the robot. Those sensors which are connected to the fixed parts of the robot provide feedback about the entire robot and its local environment. The Inertial Measurement Unit (IMU) provides feedback regarding the acceleration and angular velocity with relation to 6 different axis (x, y, z in both linear and angular planes) or Degrees of Freedom (DOF). This can be combined with information from other sensors in order to reduce the overall error in determining the robot's location with relation to its starting position.  

One such sensor is the encoder which is connected to the wheels and provides only feedback information about this actuator. These sensors are connected directly to the shaft of the motors and provide the control system with the actual distance moved by the motors. This allows adjustments to be made and each of the wheels to be maintained at a constant speed when utilised by the Proportional-Integral-Derivative(PID) controller of the robot.  

\subsection{PID Controller}\label{litreview/robotics/pid}
As mentioned above the PID controller uses feedback from the encoders to maintain a straight line of movement and maintain each of wheels at a constant velocity. The PID controller is a feedback loop which is designed to eliminate errors in the actuator systems by the mathematical description \todo{add the mathematical formula for PID add wheres also}. By tuning $ K_p $, $ T_i $ and $ T_d $ for the system in question, the error in the system can be reduced.    

Following the setting of $K_p$ to an appropriate value which gives a steady closed-loop state, $T_i$ can be used to further tune the system. The increasing of $T_i$, reduces the overshoot of the system from a state of steady oscillation but reduces the response speed of the system~\cite{chen2007linear}. $T_d$ is then used to damp any remaining oscillations to a steady line response, however this can introduce noise amplification to the system and should be used with care. 

One method of tuning a PID controller is the Ziegler-Nicholls method~\cite{ziegler1942optimum}. This principle originates from prior to autonomous robotic control and has been adapted for this application~\cite{aastrom2004revisiting}. The method experimentally finds the critical gain of $K_p$, $K_u$, from which the frequency of oscillation, $T_u$ can be found. These values can then be used to calculate $T_i$ and $T_d$ which results in a tuned system. However, many different formulae and definitions of the critical gain of $K_p$ exist in literature. This leads to the conclusion that the Ziegler-Nicholls method is mathematically imperfect and this will have to be explored experimentally during the project.     
%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1068004

\subsection{Rate Control}\label{litreview/robotics/ratecontrol}


\section{AI}\label{litreview/ai}
At the software level of control 


\section{Cooperative Robotics}\label{litreview/coop}
Co-operative robotics has varied definitions across different papers. One such
definition, which generalises co-operative behaviour in robotics, describes it as ``joint
collaborative behaviour that is directed toward some goal in which there is a
common interest or reward''~\cite{barnes1991behaviour}. This description fits the
objectives of this study more appropriately than the specific term swarm robotics, which
has a number of additional requirements, including
that problem solving should be distributed across the swarm~\cite{sahin04}.
This definition does not apply to this project, as the robots designed
here are capable of operating autonomously and will be able to solve certain tasks
individually. In this case, collaboration is used to deliver a performance improvement.
For this reason, the more general term of co-operative robotics will
be used throughout. The aim is to reduce the time taken or increase performance of the system over
a single-robot system~\cite{premvuti1990consideration}. Additionally, by creating a decentralised
and distributed system across a number of homogeneous agents, agent redundancy is introduced which
can improve the completion rate of tasks, especially in potentially volatile
environments~\cite{beckers1994local, parker95}.

Co-operative robotics goes beyond the idea
of collaborative robotics in requiring an additional aspect of intelligence in
the communication and coordination of the individual agents~\cite{cao1995cooperative}.

\section{SLAM}\label{litreview/slam}
A key challenge in mobile robotics is for the robot to know its own position in the
environment whilst still being able to build a map of its surroundings. 
This is especially true in the absence of external referencing systems
such as GPS to aid in knowing its relative position. This is known as the Simultaneous Localisation And Mapping (SLAM)
problem and has been one of the most extensively researched topics in mobile
robotics over the last two decades~\cite{grisetti2010tutorial}. As the robot's
estimate of its position is affected by both the previous state's uncertainty
and any errors in the current measurement, the uncertainties compound
over time. To rectify this, a map with distinctive landmarks can reduce its
localisation error by revisiting these known areas. This is known as loop closure.

SLAM implementations rely on sensor fusion algorithms as part of their implementation. 
This takes in readings from an array of sensors and calculates an estimated state change
based on the probability of error for each sensor. This effectively allows errors between 
multiple sensors to be cancelled out, resulting in more reliable estimates. A standard 
approach is to use sensor fusion to combine odometry readings from wheel encoders with 
acceleration information obtained from an inertial measurement unit (IMU) to correct for 
errors caused by wheels slipping and sensor imperfections.

There are a large variety of solutions to suit various system requirements, these
can be categorized as either filtering and smoothing. Filtering creates a state estimation 
using the current robot position and the map. The estimate is augmented and improved
by using the new measurements as they become available. Some popular
approaches to filtering are techniques such as Kalman filters, particle filters
and information filters. Smoothing techniques involve a full estimate of the trajectory of
the robot from all available measurements. These typically use least-square error 
minimisation techniques and are used to address the problem known as the full SLAM 
problem which attempts to map the entire path.

The state of the system is known as $x_k$ which is a function that uses the previous 
state to determine the next state. As this can not be perfectly accurate, there will 
be uncertainty in the readings that must be taken into account. As a result, $x_k$,
which is known as the motion model, is defined as
\begin{equation}
x_{k} = f(x_{k-1}, q_{k-1})\,,
\end{equation}
where $q_{k-1}$ is the randomness introduced to the system. As such, this can
also be represented by the probability distribution
\begin{equation}
x_{k} \sim p(x_{k} | x_{k-1})\,.
\end{equation}
Both of these imply that the state is stochastic and depends on the previous
state. The probability distribution emphasises that the current state is
drawn from a distribution of possible states based on the previous state. Given that a perfect sensor is not possible, the current state will also have noise
in the reading. This is known as the measurement model and can be defined as
\begin{equation}
y_{k} = h(x_{k}, r_{k})\,,
\end{equation}
where $r$ represents the uncertainty of the sensor. As
before this can be expressed as an uncertainty model:
\begin{equation}
y_{k} \sim p(y_{k} | y_{k-1})\,.
\end{equation}
It is assumed that the motion and measurement models are Markovian in that
the current state only depends on the previous state. The measurement model only
depends on the current state and no previous values.

By applying Bayes' theorem and marginalisation the current state can be described as
\begin{align}
\label{eqn:predict}
p(x_{k} | y_{1:k-1}) & = \int p(x_{k}|x_{k-1}) p(x_{k-1} | y_{1:k-1}) dx_{k-1} \\
\label{eqn:update}
p(x_{k} | y_{1:k}) &= \frac{ p(y_{k}|x_{k})p(x_{k}|x_{1:k-1})}{ p(y_{k}|y_{1:k-1})}\,.
\end{align}
Equation~\ref{eqn:predict} is known as the predict equation. By integrating over
the previous state, all potential outcomes of the state $x_k$ are
considered. Equation~\ref{eqn:update} is referred to as the update equation,
as the prediction is updated using the new measurement information~\cite{kam1997sensorfusion}.

One of the most common methods of implementing SLAM is filtering using an
Extended Kalman Filter (EKF). An EKF is an efficient, recursive filter
that estimates the state of a dynamic system from a series of noisy measurements~\cite{fox2003bayesian}.
This uses the premise of the predict and update equations as joint probability
distributions. Given variables defined on a probability space, the joint
probability distribution gives the probability that each of the variables falls in any
particular range or set. It uses these techniques to estimates a state vector containing
both the location of landmarks in the map and the robot pose~\cite{huang2007convergence}.


\section{Computer Vision}\label{litreview/cv}
Computer vision is the analysis of digital image or video data to allow a computer
system to gain a high-level understanding of the 3D environment contained within
the image\cite{CVBallard}.A common application for computer vision is the identification and classification
of objects. Identification generally involves the recognition of features of the
object and the comparison of these features and their relative positions to a
known model of the object. Classification usually involves machine learning
algorithms to build up a definition of the object based on its visible properties\cite{CVpaoletti2018new}.

Computer vision can also be used to triangulate the position of objects in the
field of view by measuring the discrepancy in the objects position in the two camera
frames given a translation matrix relating the two cameras.

Computer vision can be well integrated with SLAM providing a means of both the measure
of distance (if the CV system is bi-ocular) and the identification of distinctive
features in the environment to allow loop closure\cite{CVho2006loop}.